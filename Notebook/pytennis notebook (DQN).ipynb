{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pygame\n",
    "import sys\n",
    "import seaborn as sns\n",
    "\n",
    "from pygame.locals import *\n",
    "pygame.init()\n",
    "\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, xmin, xmax, ymin, ymax):\n",
    "        \"\"\"\n",
    "        xmin: 150,\n",
    "        xmax: 450, \n",
    "        ymin: 100, \n",
    "        ymax: 600\n",
    "        \"\"\"\n",
    "        \n",
    "        self.StaticDiscipline = {\n",
    "            'xmin': xmin, \n",
    "            'xmax': xmax, \n",
    "            'ymin': ymin, \n",
    "            'ymax': ymax\n",
    "        }\n",
    "\n",
    "    def network(self, xsource, ysource = 100, Ynew = 600, divisor = 50): #ysource will always be 100\n",
    "        \"\"\"\n",
    "        For Network A\n",
    "        ysource: will always be 100\n",
    "        xsource: will always be between xmin and xmax (static discipline)\n",
    "        \n",
    "        For Network B\n",
    "        ysource: will always be 600\n",
    "        xsource: will always be between xmin and xmax (static discipline)\n",
    "        \"\"\"\n",
    "        \n",
    "        while True:\n",
    "            ListOfXsourceYSource = []\n",
    "            Xnew = np.random.choice([i for i in range(self.StaticDiscipline['xmin'], self.StaticDiscipline['xmax'])], 1)\n",
    "            #Ynew = np.random.choice([i for i in range(self.StaticDiscipline['ymin'], self.StaticDiscipline['ymax'])], 1)\n",
    "\n",
    "            source = (xsource, ysource)\n",
    "            target = (Xnew[0], Ynew)\n",
    "\n",
    "            #Slope and intercept\n",
    "            slope = (ysource - Ynew)/(xsource - Xnew[0])\n",
    "            intercept = ysource - (slope*xsource)\n",
    "            if (slope != np.inf) and (intercept != np.inf):\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "        #print(source, target)\n",
    "        # randomly select 50 new values along the slope between xsource and xnew (monotonically decreasing/increasing)\n",
    "        XNewList = [xsource]\n",
    "\n",
    "        if xsource < Xnew:\n",
    "            differences = Xnew[0] - xsource\n",
    "            increment = differences /divisor\n",
    "            newXval = xsource\n",
    "            for i in range(divisor):\n",
    "\n",
    "                newXval += increment\n",
    "                XNewList.append(int(newXval))\n",
    "        else:\n",
    "            differences = xsource - Xnew[0]\n",
    "            decrement = differences /divisor\n",
    "            newXval = xsource\n",
    "            for i in range(divisor):\n",
    "\n",
    "                newXval -= decrement\n",
    "                XNewList.append(int(newXval))\n",
    "                \n",
    "\n",
    "        #determine the values of y, from the new values of x, using y= mx + c\n",
    "        yNewList = []\n",
    "        for i in XNewList:\n",
    "            findy = (slope * i) + intercept#y = mx + c\n",
    "            yNewList.append(int(findy))\n",
    "\n",
    "        ListOfXsourceYSource = [(x, y) for x, y in zip(XNewList, yNewList)]\n",
    "\n",
    "        return XNewList, yNewList\n",
    "    \n",
    "    \n",
    "    \n",
    "    def DefaultToPosition(self, x1, x2 = 300, divisor = 50):\n",
    "        DefaultPositionA = 300\n",
    "        DefaultPositionB = 300\n",
    "        XNewList = []\n",
    "        if x1 < x2:\n",
    "            differences = x2 - x1\n",
    "            increment = differences /divisor\n",
    "            newXval = x1\n",
    "            for i in range(divisor):\n",
    "                newXval += increment\n",
    "                XNewList.append(int(np.floor(newXval)))\n",
    "\n",
    "        else:\n",
    "            differences = x1 - x2\n",
    "            decrement = differences /divisor\n",
    "            newXval = x1\n",
    "            for i in range(divisor):\n",
    "                newXval -= decrement\n",
    "                XNewList.append(int(np.floor(newXval)))\n",
    "        return XNewList\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define DQN Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential, layers\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self):\n",
    "        self.learning_rate = 0.001\n",
    "        self.momentum = 0.95\n",
    "        self.eps_min = 0.1\n",
    "        self.eps_max = 1.0\n",
    "        self.eps_decay_steps = 2000000\n",
    "        self.replay_memory_size = 500\n",
    "        self.replay_memory = deque([], maxlen=self.replay_memory_size)\n",
    "        n_steps = 4000000 # total number of training steps\n",
    "        self.training_start = 10000 # start training after 10,000 game iterations\n",
    "        self.training_interval = 4 # run a training step every 4 game iterations\n",
    "        self.save_steps = 1000 # save the model every 1,000 training steps\n",
    "        self.copy_steps = 10000 # copy online DQN to target DQN every 10,000 training steps\n",
    "        self.discount_rate = 0.99\n",
    "        self.skip_start = 90 # Skip the start of every game (it's just waiting time).\n",
    "        self.batch_size = 100\n",
    "        self.iteration = 0 # game iterations\n",
    "        self.done = True # env needs to be reset\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        self.model = self.DQNmodel()\n",
    "        \n",
    "        return\n",
    "    \n",
    "\n",
    "\n",
    "    def DQNmodel(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, input_shape=(1,), activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(10, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    \n",
    "    def sample_memories(self, batch_size):\n",
    "        indices = np.random.permutation(len(self.replay_memory))[:batch_size]\n",
    "        cols = [[], [], [], [], []] # state, action, reward, next_state, continue\n",
    "        for idx in indices:\n",
    "            memory = self.replay_memory[idx]\n",
    "            for col, value in zip(cols, memory):\n",
    "                col.append(value)\n",
    "        cols = [np.array(col) for col in cols]\n",
    "        return (cols[0], cols[1], cols[2].reshape(-1, 1), cols[3],cols[4].reshape(-1, 1))\n",
    "\n",
    "\n",
    "    def epsilon_greedy(self, q_values, step):\n",
    "        self.epsilon = max(self.eps_min, self.eps_max - (self.eps_max-self.eps_min) * step/self.eps_decay_steps)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(10) # random action\n",
    "        else:\n",
    "            return np.argmax(q_values) # optimal action\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/qwerty/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/qwerty/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/qwerty/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/qwerty/.local/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/qwerty/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AgentA = DQN()\n",
    "AgentB = DQN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import pygame\n",
    "\n",
    "\n",
    "class pytennis:\n",
    "    def __init__(self, fps=50):\n",
    "        self.GeneralReward = False\n",
    "        self.net = Network(150, 450, 100, 600)\n",
    "        self.updateRewardA = 0\n",
    "        self.updateRewardB = 0\n",
    "        self.updateIter = 0\n",
    "        self.lossA = 0\n",
    "        self.lossB = 0\n",
    "        self.restart = False\n",
    "\n",
    "        # Testing\n",
    "        self.net = Network(150, 450, 100, 600)\n",
    "        self.NetworkA = self.net.network(\n",
    "            300, ysource=100, Ynew=600)  # Network A\n",
    "        self.NetworkB = self.net.network(\n",
    "            200, ysource=600, Ynew=100)  # Network B\n",
    "        # NetworkA\n",
    "\n",
    "        # display test plot of network A\n",
    "        #sns.jointplot(NetworkA[0], NetworkA[1])\n",
    "\n",
    "        # display test plot of network B\n",
    "        #sns.jointplot(NetworkB[0], NetworkB[1])\n",
    "\n",
    "        #self.out = self.net.DefaultToPosition(250)\n",
    "\n",
    "\n",
    "        pygame.init()\n",
    "        self.BLACK = (0, 0, 0)\n",
    "\n",
    "        self.myFontA = pygame.font.SysFont(\"Times New Roman\", 25)\n",
    "        self.myFontB = pygame.font.SysFont(\"Times New Roman\", 25)\n",
    "        self.myFontIter = pygame.font.SysFont('Times New Roman', 25)\n",
    "\n",
    "        self.FPS = fps\n",
    "        self.fpsClock = pygame.time.Clock()\n",
    "\n",
    "    def setWindow(self):\n",
    "\n",
    "        # set up the window\n",
    "        self.DISPLAYSURF = pygame.display.set_mode((600, 700), 0, 32)\n",
    "        pygame.display.set_caption(\n",
    "            'REINFORCEMENT LEARNING (Discrete Mathematics) - TABLE TENNIS')\n",
    "        # set up the colors\n",
    "        self.BLACK = (0, 0, 0)\n",
    "        self.WHITE = (255, 255, 255)\n",
    "        self.RED = (255, 0, 0)\n",
    "        self.GREEN = (0, 255, 0)\n",
    "        self.BLUE = (0, 0, 255)\n",
    "\n",
    "        return\n",
    "\n",
    "    def display(self):\n",
    "        self.setWindow()\n",
    "        self.DISPLAYSURF.fill(self.WHITE)\n",
    "        pygame.draw.rect(self.DISPLAYSURF, self.GREEN, (150, 100, 300, 500))\n",
    "        pygame.draw.rect(self.DISPLAYSURF, self.RED, (150, 340, 300, 20))\n",
    "        pygame.draw.rect(self.DISPLAYSURF, self.BLACK, (0, 20, 600, 20))\n",
    "        pygame.draw.rect(self.DISPLAYSURF, self.BLACK, (0, 660, 600, 20))\n",
    "        return\n",
    "\n",
    "    def reset(self):\n",
    "        return\n",
    "\n",
    "    def evaluate_state_from_last_coordinate(self, c):\n",
    "        \"\"\"\n",
    "        cmax: 450\n",
    "        cmin: 150\n",
    "\n",
    "        c definately will be between 150 and 450.\n",
    "        state0 - (150 - 179)\n",
    "        state1 - (180 - 209)\n",
    "        state2 - (210 - 239)\n",
    "        state3 - (240 - 269)\n",
    "        state4 - (270 - 299)\n",
    "        state5 - (300 - 329)\n",
    "        state6 - (330 - 359)\n",
    "        state7 - (360 - 389)\n",
    "        state8 - (390 - 419)\n",
    "        state9 - (420 - 450)\n",
    "        \"\"\"\n",
    "        if c >= 150 and c <= 179:\n",
    "            return 0\n",
    "        elif c >= 180 and c <= 209:\n",
    "            return 1\n",
    "        elif c >= 210 and c <= 239:\n",
    "            return 2\n",
    "        elif c >= 240 and c <= 269:\n",
    "            return 3\n",
    "        elif c >= 270 and c <= 299:\n",
    "            return 4\n",
    "        elif c >= 300 and c <= 329:\n",
    "            return 5\n",
    "        elif c >= 330 and c <= 359:\n",
    "            return 6\n",
    "        elif c >= 360 and c <= 389:\n",
    "            return 7\n",
    "        elif c >= 390 and c <= 419:\n",
    "            return 8\n",
    "        elif c >= 420 and c <= 450:\n",
    "            return 9\n",
    "\n",
    "    def evaluate_action(self, diff):\n",
    "\n",
    "        if (int(diff) <= 30):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def randomVal(self, action):\n",
    "        \"\"\"\n",
    "        cmax: 450\n",
    "        cmin: 150\n",
    "\n",
    "        c definately will be between 150 and 450.\n",
    "        state0 - (150 - 179)\n",
    "        state1 - (180 - 209)\n",
    "        state2 - (210 - 239)\n",
    "        state3 - (240 - 269)\n",
    "        state4 - (270 - 299)\n",
    "        state5 - (300 - 329)\n",
    "        state6 - (330 - 359)\n",
    "        state7 - (360 - 389)\n",
    "        state8 - (390 - 419)\n",
    "        state9 - (420 - 450)\n",
    "        \"\"\"\n",
    "        if action == 0:\n",
    "            val = np.random.choice([i for i in range(150, 180)])\n",
    "        elif action == 1:\n",
    "            val = np.random.choice([i for i in range(180, 210)])\n",
    "        elif action == 2:\n",
    "            val = np.random.choice([i for i in range(210, 240)])\n",
    "        elif action == 3:\n",
    "            val = np.random.choice([i for i in range(240, 270)])\n",
    "        elif action == 4:\n",
    "            val = np.random.choice([i for i in range(270, 300)])\n",
    "        elif action == 5:\n",
    "            val = np.random.choice([i for i in range(300, 330)])\n",
    "        elif action == 6:\n",
    "            val = np.random.choice([i for i in range(330, 360)])\n",
    "        elif action == 7:\n",
    "            val = np.random.choice([i for i in range(360, 390)])\n",
    "        elif action == 8:\n",
    "            val = np.random.choice([i for i in range(390, 420)])\n",
    "        else:\n",
    "            val = np.random.choice([i for i in range(420, 450)])\n",
    "        return val\n",
    "\n",
    "    def stepA(self, action, count=0):\n",
    "        # playerA should play\n",
    "        if count == 0:\n",
    "            self.NetworkA = self.net.network(\n",
    "                self.ballx, ysource=100, Ynew=600)  # Network A\n",
    "            self.bally = self.NetworkA[1][count]\n",
    "            self.ballx = self.NetworkA[0][count]\n",
    "\n",
    "            \n",
    "            if self.GeneralReward == True:\n",
    "                self.playerax = self.randomVal(action)\n",
    "            else:\n",
    "                self.playerax = self.ballx\n",
    "\n",
    "\n",
    "#             soundObj = pygame.mixer.Sound('sound/sound.wav')\n",
    "#             soundObj.play()\n",
    "#             time.sleep(0.4)\n",
    "#             soundObj.stop()\n",
    "\n",
    "        else:\n",
    "            self.ballx = self.NetworkA[0][count]\n",
    "            self.bally = self.NetworkA[1][count]\n",
    "\n",
    "        obsOne = self.evaluate_state_from_last_coordinate(\n",
    "            int(self.ballx))  # last state of the ball\n",
    "        obsTwo = self.evaluate_state_from_last_coordinate(\n",
    "            int(self.playerbx))  # evaluate player bx\n",
    "        diff = np.abs(self.ballx - self.playerbx)\n",
    "        obs = obsTwo\n",
    "        reward = self.evaluate_action(diff)\n",
    "        done = True\n",
    "        info = str(diff)\n",
    "\n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def stepB(self, action, count=0):\n",
    "        # playerB should play\n",
    "        if count == 0:\n",
    "            self.NetworkB = self.net.network(\n",
    "                self.ballx, ysource=600, Ynew=100)  # Network B\n",
    "            self.bally = self.NetworkB[1][count]\n",
    "            self.ballx = self.NetworkB[0][count]\n",
    "\n",
    "            \n",
    "            if self.GeneralReward == True:\n",
    "                self.playerbx = self.randomVal(action)\n",
    "            else:\n",
    "                self.playerbx = self.ballx\n",
    "\n",
    "\n",
    "#             soundObj = pygame.mixer.Sound('sound/sound.wav')\n",
    "#             soundObj.play()\n",
    "#             time.sleep(0.4)\n",
    "#             soundObj.stop()\n",
    "\n",
    "        else:\n",
    "            self.ballx = self.NetworkB[0][count]\n",
    "            self.bally = self.NetworkB[1][count]\n",
    "\n",
    "        obsOne = self.evaluate_state_from_last_coordinate(\n",
    "            int(self.ballx))  # last state of the ball\n",
    "        obsTwo = self.evaluate_state_from_last_coordinate(\n",
    "            int(self.playerax))  # evaluate player bx\n",
    "        diff = np.abs(self.ballx - self.playerax)\n",
    "        obs = obsTwo\n",
    "        reward = self.evaluate_action(diff)\n",
    "        done = True\n",
    "        info = str(diff)\n",
    "\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    \n",
    "    def computeLossA(self, reward):\n",
    "        if reward == 0:\n",
    "            self.lossA += 1\n",
    "        else:\n",
    "            self.lossA += 0\n",
    "        return\n",
    "\n",
    "    def computeLossB(self, reward):\n",
    "        if reward == 0:\n",
    "            self.lossB += 1\n",
    "        else:\n",
    "            self.lossB += 0\n",
    "        return\n",
    "\n",
    "    def render(self):\n",
    "        # diplay team players\n",
    "        self.PLAYERA = pygame.image.load('images/cap.jpg')\n",
    "        self.PLAYERA = pygame.transform.scale(self.PLAYERA, (50, 50))\n",
    "        self.PLAYERB = pygame.image.load('images/cap.jpg')\n",
    "        self.PLAYERB = pygame.transform.scale(self.PLAYERB, (50, 50))\n",
    "        self.ball = pygame.image.load('images/ball.png')\n",
    "        self.ball = pygame.transform.scale(self.ball, (15, 15))\n",
    "\n",
    "        self.playerax = 150\n",
    "        self.playerbx = 250\n",
    "\n",
    "        self.ballx = 250\n",
    "        self.bally = 300\n",
    "\n",
    "        count = 0\n",
    "        nextplayer = 'A'\n",
    "        # player A starts by playing with state 0\n",
    "        obsA, rewardA, doneA, infoA = 0, False, False, ''\n",
    "        obsB, rewardB, doneB, infoB = 0, False, False, ''\n",
    "        stateA = 0\n",
    "        stateB = 0\n",
    "        next_stateA = 0\n",
    "        next_stateB = 0\n",
    "\n",
    "        actionA = 0\n",
    "        actionB = 0\n",
    "\n",
    "        iterations = 20000\n",
    "        iteration = 0\n",
    "        restart = False\n",
    "\n",
    "        while iteration < iterations:\n",
    "            \n",
    "            self.display()\n",
    "            self.randNumLabelA = self.myFontA.render(\n",
    "                'A (Win): '+str(self.updateRewardA) + ', A(loss): '+str(self.lossA), 1, self.BLACK)\n",
    "            self.randNumLabelB = self.myFontB.render(\n",
    "                'B (Win): '+str(self.updateRewardB) + ', B(loss): ' + str(self.lossB), 1, self.BLACK)\n",
    "            self.randNumLabelIter = self.myFontIter.render(\n",
    "                'Iterations: '+str(self.updateIter), 1, self.BLACK)\n",
    "            \n",
    "            if nextplayer == 'A':\n",
    "                \n",
    "                if count == 0:\n",
    "                    # Online DQN evaluates what to do\n",
    "                    q_valueA = AgentA.model.predict([stateA])\n",
    "                    actionA = AgentA.epsilon_greedy(q_valueA, iteration)\n",
    "\n",
    "                    # Online DQN plays\n",
    "                    obsA, rewardA, doneA, infoA = self.stepA(\n",
    "                        action=actionA, count=count)\n",
    "                    next_stateA = actionA\n",
    "\n",
    "                    # Let's memorize what just happened\n",
    "                    AgentA.replay_memory.append(\n",
    "                        (stateA, actionA, rewardA, next_stateA, 1.0 - doneA))\n",
    "                    stateA = next_stateA\n",
    "\n",
    "                \n",
    "\n",
    "                elif count == 49:\n",
    "                    \n",
    "\n",
    "                    # Online DQN evaluates what to do\n",
    "                    q_valueA = AgentA.model.predict([stateA])\n",
    "                    actionA = AgentA.epsilon_greedy(q_valueA, iteration)\n",
    "                    obsA, rewardA, doneA, infoA = self.stepA(\n",
    "                        action=actionA, count=count)\n",
    "                    next_stateA = actionA\n",
    "\n",
    "\n",
    "                    self.updateRewardA += rewardA\n",
    "                    self.computeLossA(rewardA)\n",
    "\n",
    "                    # Let's memorize what just happened\n",
    "                    AgentA.replay_memory.append(\n",
    "                        (stateA, actionA, rewardA, next_stateA, 1.0 - doneA))\n",
    "                    \n",
    "\n",
    "                    # restart the game if player A fails to get the ball, and let B start the game\n",
    "                    if rewardA == 0:\n",
    "                        self.restart = True\n",
    "                        time.sleep(0.5)\n",
    "                        nextplayer = 'B'\n",
    "                        self.GeneralReward = False\n",
    "                    else:\n",
    "                        self.restart = False\n",
    "                        self.GeneralReward = True\n",
    "\n",
    "                    # Sample memories and use the target DQN to produce the target Q-Value\n",
    "                    X_state_val, X_action_val, rewards, X_next_state_val, continues = (\n",
    "                        AgentA.sample_memories(AgentA.batch_size))\n",
    "                    next_q_values = AgentA.model.predict([X_next_state_val])\n",
    "                    max_next_q_values = np.max(\n",
    "                        next_q_values, axis=1, keepdims=True)\n",
    "                    y_val = rewards + continues * AgentA.discount_rate * max_next_q_values\n",
    "\n",
    "                    # Train the online DQN\n",
    "                    AgentA.model.fit(X_state_val, tf.keras.utils.to_categorical(\n",
    "                        X_next_state_val, num_classes=10), verbose=0)\n",
    "\n",
    "                    nextplayer = 'B'\n",
    "                    self.updateIter += 1\n",
    "\n",
    "                    count = 0\n",
    "                    # evaluate A\n",
    "                    \n",
    "                else:\n",
    "                    # Online DQN evaluates what to do\n",
    "                    q_valueA = AgentA.model.predict([stateA])\n",
    "                    actionA = AgentA.epsilon_greedy(q_valueA, iteration)\n",
    "\n",
    "                    # Online DQN plays\n",
    "                    obsA, rewardA, doneA, infoA = self.stepA(\n",
    "                        action=actionA, count=count)\n",
    "                    next_stateA = actionA\n",
    "\n",
    "                    # Let's memorize what just happened\n",
    "                    AgentA.replay_memory.append(\n",
    "                        (stateA, actionA, rewardA, next_stateA, 1.0 - doneA))\n",
    "                    stateA = next_stateA\n",
    "                    \n",
    "                \n",
    "                 \n",
    "                if nextplayer == 'A':\n",
    "                    count += 1\n",
    "                else:\n",
    "                    count = 0\n",
    "\n",
    "            else:\n",
    "                if count == 0:\n",
    "                    # Online DQN evaluates what to do\n",
    "                    q_valueB = AgentB.model.predict([stateB])\n",
    "                    actionB = AgentB.epsilon_greedy(q_valueB, iteration)\n",
    "\n",
    "                    # Online DQN plays\n",
    "                    obsB, rewardB, doneB, infoB = self.stepB(\n",
    "                        action=actionB, count=count)\n",
    "                    next_stateB = actionB\n",
    "\n",
    "                    # Let's memorize what just happened\n",
    "                    AgentB.replay_memory.append(\n",
    "                        (stateB, actionB, rewardB, next_stateB, 1.0 - doneB))\n",
    "                    stateB = next_stateB\n",
    "                \n",
    "\n",
    "                elif count == 49:\n",
    "\n",
    "                    # Online DQN evaluates what to do\n",
    "                    q_valueB = AgentB.model.predict([stateB])\n",
    "                    actionB = AgentB.epsilon_greedy(q_valueB, iteration)\n",
    "\n",
    "                    # Online DQN plays\n",
    "                    obs, reward, done, info = self.stepB(\n",
    "                        action=actionB, count=count)\n",
    "                    next_stateB = actionB\n",
    "\n",
    "                    # Let's memorize what just happened\n",
    "                    AgentB.replay_memory.append(\n",
    "                        (stateB, actionB, rewardB, next_stateB, 1.0 - doneB))\n",
    "                    \n",
    "                    stateB = next_stateB\n",
    "                    self.updateRewardB += rewardB\n",
    "                    self.computeLossB(rewardB)\n",
    "\n",
    "                    # restart the game if player A fails to get the ball, and let B start the game\n",
    "                    if rewardB == 0:\n",
    "                        self.restart = True\n",
    "                        time.sleep(0.5)\n",
    "                        self.GeneralReward = False\n",
    "                        nextplayer = 'A'\n",
    "                    else:\n",
    "                        self.restart = False\n",
    "                        self.GeneralReward = True\n",
    "\n",
    "                    # Sample memories and use the target DQN to produce the target Q-Value\n",
    "                    X_state_val, X_action_val, rewards, X_next_state_val, continues = (\n",
    "                        AgentB.sample_memories(AgentB.batch_size))\n",
    "                    next_q_values = AgentB.model.predict([X_next_state_val])\n",
    "                    max_next_q_values = np.max(\n",
    "                        next_q_values, axis=1, keepdims=True)\n",
    "                    y_val = rewards + continues * AgentB.discount_rate * max_next_q_values\n",
    "\n",
    "                    # Train the online DQN\n",
    "                    AgentB.model.fit(X_state_val, tf.keras.utils.to_categorical(\n",
    "                        X_next_state_val, num_classes=10), verbose=0)\n",
    "\n",
    "                    nextplayer = 'A'\n",
    "                    self.updateIter += 1\n",
    "                    # evaluate B\n",
    "                    \n",
    "                        \n",
    "                else:\n",
    "                    # Online DQN evaluates what to do\n",
    "                    q_valueB = AgentB.model.predict([stateB])\n",
    "                    actionB = AgentB.epsilon_greedy(q_valueB, iteration)\n",
    "\n",
    "                    # Online DQN plays\n",
    "                    obsB, rewardB, doneB, infoB = self.stepB(\n",
    "                        action=actionB, count=count)\n",
    "                    next_stateB = actionB\n",
    "\n",
    "                    # Let's memorize what just happened\n",
    "                    AgentB.replay_memory.append(\n",
    "                        (stateB, actionB, rewardB, next_stateB, 1.0 - doneB))\n",
    "                    tateB = next_stateB\n",
    "                    \n",
    "                    \n",
    "                if nextplayer == 'B':\n",
    "                    count += 1\n",
    "                else:\n",
    "                    count = 0\n",
    "\n",
    "                \n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "            # CHECK BALL MOVEMENT\n",
    "            self.DISPLAYSURF.blit(self.PLAYERA, (self.playerax, 50))\n",
    "            self.DISPLAYSURF.blit(self.PLAYERB, (self.playerbx, 600))\n",
    "            self.DISPLAYSURF.blit(self.ball, (self.ballx, self.bally))\n",
    "            self.DISPLAYSURF.blit(self.randNumLabelA, (300, 630))\n",
    "            self.DISPLAYSURF.blit(self.randNumLabelB, (300, 40))\n",
    "            self.DISPLAYSURF.blit(self.randNumLabelIter, (50, 40))\n",
    "\n",
    "            # update last coordinate\n",
    "            # self.lastxcoordinate = self.ballx\n",
    "\n",
    "            pygame.display.update()\n",
    "            self.fpsClock.tick(self.FPS)\n",
    "\n",
    "            for event in pygame.event.get():\n",
    "\n",
    "                if event.type == QUIT:\n",
    "                    AgentA.model.save('AgentA.h5')\n",
    "                    AgentB.model.save('AgentB.h5')\n",
    "                    pygame.quit()\n",
    "                    sys.exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_1_input to have 2 dimensions, but got array with shape ()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d04c97e33349>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtennis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpytennis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtennis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtennis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-d5acf3436ae2>\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                     \u001b[0;31m# Online DQN evaluates what to do\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m                     \u001b[0mq_valueA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgentA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstateA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m                     \u001b[0mactionA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgentA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_greedy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_valueA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/qwerty/.local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/qwerty/.local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/qwerty/.local/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_1_input to have 2 dimensions, but got array with shape ()"
     ]
    }
   ],
   "source": [
    "tennis = pytennis(fps = 50)\n",
    "tennis.reset()\n",
    "tennis.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
